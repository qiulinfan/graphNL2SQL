\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}
\usepackage{booktabs} 
\usepackage{minted}
\usepackage{float}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{listings}
\usepackage{enumitem}
\setlist[itemize]{itemsep=1pt, topsep=3pt}

% Define code style
\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{gray!10},   % Background color
    commentstyle=\color{green},        % Comment style
    keywordstyle=\color{blue},         % Keyword style
    numberstyle=\tiny\color{gray},     % Line numbers style
    stringstyle=\color{orange},        % String style
    basicstyle=\ttfamily\footnotesize, % Basic font style
    breaklines=true,                   % Line breaking
    frame=single,                      % Frame around the code
    % numbers=left,                      % Line numbers on the left
    captionpos=b,                      % Caption at the bottom
    showstringspaces=false             % Hide spaces in strings
}

\lstset{style=pythonstyle}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb} 

% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\makeatletter
\renewcommand\cite{\citep}
\makeatother
\title{Fine-tuning Small LLMs with Graph-Modeled Schemas for Multi-table NL2SQL}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Zhanhao Liu \\
  University of Michigan \\
  Ann Arbor, MI \\
  \texttt{zhanhaol@umich.edu} \\\And
  Qiulin Fan \\
  University of Michigan \\
  Ann Arbor, MI \\
  \texttt{
rynnefan@umich.edu} \\}


\begin{document}
\maketitle

\section{Introduction}

\subsection{Background and Motivation}
Early research on Natural Language to SQL (NL2SQL) before the LLM era relied on task-specific encoder–decoder architectures such as IRNet\cite{IRNet2019}, Seq2SQL\cite{Seq2SQL2017}, SQLNet\cite{SQLnet2017}, SyntaxSQLNet\cite{SyntaxSQLNet2018}, and RAT-SQL\cite{RatSQL2021}. These models achieved remarkable progress on benchmark datasets but remain fundamentally limited, since their parameters are typically under the order of tens of millions, without any large-scale pretraining on natural language corpora. As a result:
\begin{itemize}
    \item They lack general language modeling ability and exhibit poor few-shot generalization, upon domains and databases they have not seen, even simple ones.
    \item  When confronted with complex queries (e.g. multi-table Join), these models often fail to transfer learned knowledge.
\end{itemize} 
In our baseline evaluations, we observed that such models, if not fine-tuned on the specific dataset, perform poorly even on simple benchmarks such as WikiSQL.

Fortunately, with the emergence of large-scale LLMs, complex NL2SQL tasks has become more feasible. General-purpose models such as GPT-5-Codex demonstrate strong natural language to code reasoning. However, this shift also exposes practical limitations. Research this year\cite{LearNAT2025} points out current SOTA methods largely depend on closed-source LLMs combined with prompt engineering, while open-source models still struggle on complex queries involving multiple joins or nested subqueries. In addition, large LLMs entail massive computational cost and memory consumption, making them impractical for domain-specific fine-tuning.

Therefore, our project aims to explore how a small-parameter LLM (3 \textasciitilde
8 B) can be fine-tuned to achieve strong NL2SQL performance comparable to large models in handling of complex tasks, but with a fraction of the computational cost. 

While LearNAT\cite{LearNAT2025} framework employs task decomposition, abstract syntax tree (AST) encoding and margin-aware reinforcement learning, our work instead focuses on graph-based schema modeling to strengthen the model’s structural reasoning. We hypothesize that incorporating graph representations provides a complementary advantage—helping smaller LLMs internalize multi-table relationships and bridge the gap between structural understanding and natural-language semantics.


% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\textwidth]{image/bird.png}
%     \caption{An example text-to-SQL task from the BIRD dataset. The query involves multiple tables, each with columns containing realistic values (not fully shown here).}
%     \label{fig:bird}
% \end{figure*}

\subsection{Task Definition}

The specific NLP task this project will address is multi-table Natural Language to SQL (NL2SQL) generation. The task of multi-table NL2SQL generation is to take a natural language query as input and automatically produce a syntactically correct and semantically accurate SQL query that retrieves the correct result from a relational database containing multiple interconnected tables. 
\begin{itemize}
    \item Input: a pair of $(\text{SQL schema}, \text{NL query})$. The schema may contain multiple tables with foreign key relationships. (Note: A foreign key is a column, or set of columns, in one table that refers to the primary key of another table.)
    \item Output: a structured SQL query that can be executed on the target database to return the intended result.
\end{itemize}

% \textbf{Current Idea}:

% Our current approach begins with generating a graph representation of the database. This graph is provided to the model alongside the database schema and the natural language (NL) query, both during training and inference.

% To construct the complete graph, we first construct a graph for each table in the database individually, and then connect these graphs by adding edges between tables. The connections will be established either through explicit foreign key relationships provided in the schema or through semantic understanding of column names (e.g., “Birthday” in one table likely corresponds to “DOB” in another). The graphs for individual tables will remain relatively sparse. To illustrate this approach, we will provide an example of our intended graph structure.
% \begin{center}
%     \includegraphics[width=0.5\textwidth]{image/graph.png}
% \end{center}

% Now the graph captures richer representations of the relationships between tables.  We will incorporate this graph representation alongside the original inputs (schema and query) to further train the NL2SQL model using reinforcement learning.
% The training process will leverage three types of feedback: (1) compilability, measuring whether the generated SQL can be executed successfully on the database, (2) accuracy, assessing whether the query retrieves the correct results, and (3) optimization with respect to time and space complexity. 

% Our proposed idea is based on the assumption that incorporating graph representations during training will help the model better capture relationships between tables, thereby enabling it to generate more accurate and reasonable SQL queries that perform "join" between tables.

\section{Data}
\subsection{WikiSQL Dataset}

The \textbf{WikiSQL} dataset, released by Salesforce along with their paper \textit{Seq2SQL: Generating Structured Queries from Natural Language Using Reinforcement Learning}, is one of the earliest and most widely used benchmarks for NL2SQL models. It is easy to extract and work with, containing 15{,}878 natural language (NL) to SQL pairs along with 4{,}550 data tables on which queries can be executed. 

Using this dataset, we evaluated our generated SQL queries in two ways: 1.by directly comparing the predicted SQL with the gold (reference) SQL.2. by executing both queries on the actual table using DuckDB and comparing their outputs.

However, the WikiSQL dataset focuses solely on generating SQL queries over a single table. It does not include joins or nested queries, making it relatively simple. Since our project aims to improve models’ ability to generate SQL across multiple tables and databases---which is a key limitation of many current NL2SQL systems---we also incorporated the Spider dataset for a more realistic and challenging evaluation.
\begin{center}
\fbox{
\parbox{0.95\linewidth}{
\textbf{Example} \\
\textbf{Question:} Return the dates of birth for entrepreneurs who have either the investor Simon Woodroffe or Peter Jones. \\
\textbf{GOLD SQL:} \texttt{SELECT T2.Date\_of\_Birth FROM entrepreneur AS T1 JOIN people AS T2 ON T1.People\_ID = T2.People\_ID WHERE T1.Investor = 'Simon Woodroffe' OR T1.Investor = 'Peter Jones'}
}
}
\end{center}

\begin{table*}[htbp]
\centering
\caption{Truncated table for Example 1 from Wikisql dataset (partial columns shown)}
\begin{tabular}{lcccc}
\hline
\textbf{Institution} & \textbf{Wins} & \textbf{Losses} & \textbf{Home Wins} & \textbf{Home Losses} \\
\hline
Boston College Eagles & 6 & 1 & 3 & 1 \\
Clemson Tigers & 9 & 5 & 4 & 3 \\
Duke Blue Devils & 12 & 2 & 5 & 0 \\
Florida State Seminoles & 6 & 8 & 4 & 3 \\
Georgia Tech Yellow Jackets & 4 & 9 & 3 & 2 \\
Maryland Terrapins & 10 & 4 & 5 & 1 \\
... & ... & ... & ... & ... \\
\hline
\end{tabular}
\label{tab:wikisql}
\end{table*}




\subsection{Spider Dataset}

The \textbf{Spider} dataset, created by Yale University (\textit{Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task}), covers a wide range of domains and includes many complex SQL queries involving multi-table joins and nested structures. Its development set contains 1{,}023 questions and gold SQL queries across 166 databases, each provided with full schema information.

Compared to WikiSQL, Spider introduces substantially higher difficulty, as queries often involve multiple tables, foreign key reasoning, and advanced SQL operators such as GROUP BY, ORDER BY, and nested subqueries. This makes Spider a more realistic benchmark for assessing compositional generalization and schema understanding.

To use this dataset, we preprocessed each database schema into textual form by concatenating table and column names along with their relationships, then reorganized the format for compatibility with our model. We evaluated our system against the \texttt{Text2SQL-1.5B} model and achieved an exact match accuracy of 41.3\% on SQL generation. While the performance demonstrates reasonable generalization, further improvements may require enhanced schema linking and reasoning across complex relational structures.

\begin{lstlisting}[caption={Example from Spider dataset}, label={lst:spider_example}]
{
    "db_id": "entrepreneur",
    "query": "SELECT T2.Date_of_Birth   FROM entrepreneur AS T1 
              JOIN people AS T2 ON T1.People_ID = T2.People_ID 
              WHERE T1.Investor = 'Simon Woodroffe' 
                 OR T1.Investor = 'Peter Jones'",
    "question": "Return the dates of birth for entrepreneurs who have 
    either the investor Simon Woodroffe or Peter Jones."
}
\end{lstlisting}

For example of Spider schema structure, please see Appendix \ref{app:exp}.

\section{Related Work}
This section summarizes three lines of related work that are closely connected to our project: (1) traditional pre-LLM models for NL2SQL; (2) structurally enhanced models that incorporate schema and syntax information; and (3) recent LLM-based pipelines that achieve state-of-the-art performance through task decomposition and reinforcement learning.

\paragraph{Pre-LLM Models for NL2SQL.}
Early research on text-to-SQL adopted sequence-to-sequence architectures without large-scale language pretraining. Seq2SQL \cite{Seq2SQL2017} first introduced a reinforcement-learning objective to directly optimize execution accuracy rather than token-level similarity, but the approach suffered from unstable reward signals. SQLNet \cite{SQLnet2017} improved upon this by using a sketch-based decoder to avoid reinforcement learning altogether, achieving more stable training on the WikiSQL dataset. However, both methods focused on single-table scenarios and lacked the ability to generalize to complex, cross-domain databases. 

\paragraph{Structure-Enhanced Text-to-SQL Models.}
Subsequent work emphasized the importance of modeling structural dependencies in database schemas and SQL syntax. SyntaxSQLNet \cite{SyntaxSQLNet2018} leveraged a syntax tree decoder to enforce SQL grammar constraints, enabling generation of compositional queries. IRNet \cite{IRNet2019} introduced intermediate representations to capture the semantic alignment between natural language and database elements, improving cross-domain transfer. RAT-SQL \cite{RatSQL2021} further advanced this direction by proposing a relation-aware transformer encoder that explicitly encodes schema linking and foreign-key relations. Despite these advances, all such models remain relatively small in scale (tens of millions of parameters) and lack general-purpose language understanding, resulting in weak few-shot generalization and limited performance on multi-table join queries.

\paragraph{LLM-based NL2SQL and LearNAT.}
In the era of large language models, NL2SQL research has shifted toward leveraging general-purpose LLMs with prompt-based reasoning. LearNAT \cite{LearNAT2025}, a framework that substantially improves the NL2SQL performance of open-source LLMs through task decomposition and reinforcement learning. LearNAT decomposes complex SQL generation into structured subtasks using Abstract Syntax Trees (ASTs), combining three key components: (1) an AST-guided decomposition synthesis procedure that generates valid subtasks, (2) margin-aware reinforcement learning that optimizes multi-step reasoning with AST-based preference signals, and (3) adaptive demonstration retrieval during inference. Experiments on Spider and BIRD show that LearNAT enables a 7B-parameter open-source model to approach GPT-4-level accuracy, demonstrating the effectiveness of decomposition and RL-based supervision.

While LearNAT focuses on task decomposition through AST structures, our project extends this idea in a complementary direction by incorporating graph-based schema representations. Instead of decomposing SQL syntax, we explicitly model relational structures within the database schema as a graph to strengthen the model’s understanding of multi-table connections. This approach aims to enhance small-parameter LLMs’ structural reasoning ability in NL2SQL tasks without the computational overhead of large-scale reinforcement learning.



\section{Methodology}

\subsection{Graph-Modeling Designs}

Given a relational database schema that contains multiple tables, foreign keys, and column names, the central question is how to transform this schema into a graph structure that an LLM can effectively understand. 

Basic structures include Table-level Graph: Nodes correspond to tables, and edges represent explicit foreign-key relationships.
\begin{lstlisting}[caption={Example of Table-Level Graph}]
Nodes: [Student, Course, Department]
Edges: Student -- Course (student_id)
       Course -- Department (dept_id)
\end{lstlisting}
and Column-level Graph: Nodes correspond to columns. Edges are constructed between foreign key columns and their referenced primary keys, and columns within the same table (intra-table edges).
\begin{lstlisting}[caption={Example of Column-Level Graph}]
[Student.id] -- [Course.student_id]
[Student.name] -- (intra) -- [Student.age]
\end{lstlisting}

\paragraph{Basic Design: Table\&Column-level Hybrid Graph.}

We make hybrid of these two structures, let both tables and columns be represented as nodes. Edges include:
\begin{itemize}
    \item table column containment edges
    \item foreign key connections between columns
    \item table–table edges for cross-table relationships
\end{itemize}
\begin{lstlisting}[caption={Hybrid Graph}]
Table: Student
  ↳ id, name, age
Table: Course
  ↳ cid, title, student_id
Edges: Student.id -- Course.student_id
\end{lstlisting}

This design naturally expresses hierarchical structure and supports cross-table reasoning such as “which tables contain columns related to Student.” It strikes a balance between expressiveness and length, and is used as our main experimental design. Meanwhile, we plan to add more experimental features:

\paragraph{Extension 1: Semantic Edge.}
Built on top of the hybrid structure, this extension adds semantic edges between columns or tables whose names are semantically similar. We compute embedding similarity between names and connect pairs whose cosine similarity exceeds a threshold (e.g., 0.8):

\begin{lstlisting}[caption={Semantic Edge}]
[Birthday] ↔ [DOB]
[Department] ↔ [Dept]
\end{lstlisting}

This enriches schema understanding by introducing latent semantic connections that are not explicitly defined in the database schema, allowing the model to generalize across naming variations. However, the added edges may also increase graph density and introduce potential noise. We use this variant to evaluate the trade-off between structural richness and model robustness.

\paragraph{Extension 2: Typed Graph.}
As a further extension, all nodes and edges are annotated with type labels to explicitly distinguish their relational roles:
\begin{itemize}
  \item Edge types: \texttt{foreign\_key}, \texttt{intra\_table}, \texttt{semantic\_similar}.
  \item Node types: \texttt{table}, \texttt{column}, \texttt{primary\_key}, \texttt{foreign\_key}.
\end{itemize}

An example of the linearized input is shown below:
\begin{lstlisting}[caption={Typed Graph}]
[table] Student
  [column_primary] id
  [column] name
  [column] age
[foreign_key_edge] Student.id -> Course.student_id
[semantic_edge] Birthday ~ DOB
\end{lstlisting}

The typed graph provides the most expressive structural representation, enabling the LLM to differentiate between relationship types explicitly. However, this design also increases input length and may raise computational cost during fine-tuning. It will be evaluated as an advanced configuration in our ablation experiments.

In future experiments, we will operate on the fundamental graph design, together with two extensions, combining the results for comparisons.


\subsection{Graph Encoding Methods}

We will try two methods for integrating graph structures into LLM inputs and choose the better one in practice. 

\paragraph{Text Linearization}
Graph structures are converted into textual prompts that describe table and column relations explicitly:
\begin{lstlisting}[caption={Text Linearization}]
Schema Graph:
Table: Student(id, name, age)
Table: Course(cid, title)
Foreign Key: Student.id -> Course.student_id
Semantic Link: (DOB) ≈ (Birthday)

Question: "List the names of students taking math."
\end{lstlisting}

This approach maintains full compatibility with existing LLM tokenizers and training pipelines.

\paragraph{Graph Embedding.}
A lightweight graph encoder such as a GNN encoder encodes the schema graph into a dense embedding vector, which is injected into the LLM using parameter-efficient methods such as LoRA.
% The graph encoder can be frozen or jointly fine-tuned, depending on computational budget.
For practical fine-tuning, we use tokenized tag-style formatting for schema representation:
\begin{lstlisting}[caption={Graph Embedding}]
[Table] Student [Columns] id(PK), name, age
[Table] Course [Columns] cid(PK), title, student_id(FK->Student.id)
[Relation] Student.id = Course.student_id
\end{lstlisting}

Currently we are using Text Linearization.

\subsection{LoRA Fine-tuning}
We will fine-tune a small-parameter open-source LLM using LoRA. LoRA introduces low-rank matrices to the attention and feed-forward layers, allowing the model to learn task-specific adaptations while keeping the original weights frozen.

Candidate models include Llama-8B-Instruct, Mistral-7B, Phi-3-Mini (3.8B), and Qwen-1.5-7B.

The training configuration is as follows as an example:

\begin{itemize}
  \item Base model: Llama-8B
  \item Fine-tuning method: LoRA 
  \item Rank $r$: 32, $\alpha$: 32, dropout: 0.05
  \item Learning rate: 5e-4
  \item Sequence length: 2048 tokens
  \item Epochs: 5 on Spider 
  \item Optimizer: AdamW
\end{itemize}

We have not run the training since we are still applying for free GPUs. If free GPUs are not powerful enough, we consider renting GPUs on Runpod. Moreover, the configuration is to be modified later in practical training.

\subsection{Reinforcement Learning Discussion}

Full RL training in NL2SQL faces several practical challenges: high execution cost (each SQL must be run on a database), sparse rewards, unstable gradients, and large computational overhead. As reported by prior work\cite{LearNAT2025}, RL often yields marginal improvements (1 to 3\%) with significant complexity.

To address these issues, we would consider not using full RL, but adopt Execution-Guided Decoding (EGD) as an alternative. EGD applies execution feedback only at inference time:
\begin{enumerate}
  \item Use beam search to generate top-$k$ SQL candidates.
  \item Execute each query on the database.
  \item Select the highest-probability query that is executable and returns the correct result.
\end{enumerate}
Mentioned by \cite{egd}, This approach provides 3 to 5\% improvement in execution accuracy without additional training cost, serving as a practical, execution-aware decoding strategy.


\subsection{Final Problem Formulation}
\paragraph{Graph Representation of Schema}
Let a relational schema be modeled as a typed multi-relational graph
\[
\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{R}),
\]
where each node \(v \in \mathcal{V}\) is either a table or a column, and each edge \(e = (u, v, r) \in \mathcal{E}\) carries a relation type \(r \in \mathcal{R}\). We consider a set of edge types
\begin{align*}
    \mathcal{R} = \{\text{table\_column}, \text{foreign\_key}, \\ \text{intra\_table},  \text{semantic\_similar}\}
\end{align*}

Typed adjacency can be represented by a stack of binary matrices \(A^{(r)} \in \{0,1\}^{|\mathcal{V}|\times|\mathcal{V}|}\), one per relation \(r\).

For the semantic extension, we induce edges by embedding similarity. Let \(e(v)\) denote the textual name of node \(v\) and \(\mathbf{z}(v) = \mathrm{Enc}(e(v))\) be its embedding. A semantic edge is added between \(u\) and \(v\) if
\[
\frac{\mathbf{z}(u)^\top \mathbf{z}(v)}{\|\mathbf{z}(u)\|\,\|\mathbf{z}(v)\|} \ge \tau,
\]
with threshold \(\tau \in (0,1)\) is a hyperparameter we will set later.
Suppose usiing text linearization for integration, the training input is the concatenation
\[
x = \mathrm{Concat}\big(q,\, \mathrm{SchemaText},\, s\big),
\]
where \(q\) is the natural language question and \(\mathrm{SchemaText}\) is a plain-text schema description.

% \paragraph{Graph embedding.}
% Alternatively, a graph encoder \(g_\phi\) produces a pooled vector \(\mathbf{h}_\mathcal{G} = g_\phi(\mathcal{G}) \in \mathbb{R}^{d_h}\). We inject \(\mathbf{h}_\mathcal{G}\) into an LLM by prefix tokens or adapter features.

\paragraph{Generation Model}
Let \(p_\theta\) be a small-parameter LLM with parameters \(\theta\). Given input \(x\) and optional graph feature \(\mathbf{h}_\mathcal{G}\), the model generates a SQL token sequence \(y = (y_1,\dots,y_T)\) with
\[
p_\theta(y \mid x, \mathbf{h}_\mathcal{G}) = \prod_{t=1}^{T} p_\theta\big(y_t \mid y_{<t}, x, \mathbf{h}_\mathcal{G}\big).
\]

\paragraph{Supervised Fine-tuning Objective}
Given a dataset \(\mathcal{D} = \{(q_i, \mathcal{G}_i, y_i^\star)\}_{i=1}^N\), we minimize the token-level negative log-likelihood
\[
\mathcal{L}_{\text{SFT}}(\theta) = - \sum_{i=1}^{N} \sum_{t=1}^{T_i} \log p_\theta\big(y_{i,t}^\star \mid y_{i,<t}^\star,\, x_i,\, \mathbf{h}_{\mathcal{G}_i}\big).
\]
% Optionally, when semantic edges are used, we regularize the graph density to control noise
% \[
% \mathcal{L}_{\text{sem}} = \lambda_{\text{deg}} \cdot \frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \mathrm{deg}_{\text{semantic}}(v),
% \]
% and train with \(\mathcal{L}_{\text{SFT}} + \mathcal{L}_{\text{sem}}\).

\paragraph{LoRA Parameterization}
We adopt LoRA for parameter-efficient tuning. For a weight matrix \(W \in \mathbb{R}^{m \times n}\) in attention or MLP blocks, we learn a low-rank update
\[
W' = W + \Delta W,\quad \Delta W = B A,
\]
where \(A \in \mathbb{R}^{r \times n}\), \(B \in \mathbb{R}^{m \times r}\), and \(r \ll \min(m,n)\). Only \(A, B\) are trainable while \(W\) is frozen.
% With QLoRA, \(W\) is quantized for memory efficiency while \(A, B\) remain in higher precision.

\paragraph{Execution-aware Inference via EGD}
This part has not been set up, but we will consider it later.
Define an execution oracle \(\mathcal{E}(y)\) that returns a tuple \((c, r)\), where \(c \in \{0,1\}\) indicates compilability and \(r \in \{0,1\}\) indicates execution correctness against the gold answer. At inference time we generate a candidate set
\[
\mathcal{C}_k = \mathrm{BeamSearch}\big(p_\theta(\cdot \mid x, \mathbf{h}_\mathcal{G}), k\big),
\]
evaluate \(\{ \mathcal{E}(y) : y \in \mathcal{C}_k \}\), and select
\[
\hat{y} \in \arg\max_{y \in \mathcal{C}_k} \Big( r(y),\, c(y),\, \log p_\theta(y \mid x, \mathbf{h}_\mathcal{G}) \Big),
\]
lexicographically by \(r\) then \(c\) then model score. This execution-guided decoding improves execution accuracy without modifying training.

\paragraph{Evaluation Metrics}
We report exact match accuracy
\[
\mathrm{EM} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\big[y_i = y_i^\star\big],
\]
and execution accuracy
\[
\mathrm{EX} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}\big[\mathcal{E}(y_i) = (1,1)\big].
\]
For multi-table reasoning we additionally measure join correctness
\[
\mathrm{JAcc} = \frac{1}{N} \sum_{i=1}^{N} \frac{\big|\mathrm{Joins}(y_i) \cap \mathrm{Joins}(y_i^\star)\big|}{\big|\mathrm{Joins}(y_i) \cup \mathrm{Joins}(y_i^\star)\big|}.
\]

% \paragraph{Ablation Protocol over Graph Variants}
% Let \(\mathcal{G}^{\text{base}}\) be the hybrid graph used as the baseline. We evaluate extensions by toggling edge sets
% \[
% \mathcal{E}^{\text{sem}} = \{(u, v, \text{semantic\_similar})\},\quad
% \mathcal{E}^{\text{typed}} = \bigcup_{r \in \mathcal{R}} \mathcal{E}^{(r)},
% \]
% and compare
% \[
% \mathcal{G}^{\text{base}},\quad
% \mathcal{G}^{\text{base}} \cup \mathcal{E}^{\text{sem}},\quad
% \mathcal{G}^{\text{base}} \text{ with typed labels},
% \]
% under identical fine-tuning schedules. Statistical significance is tested by paired bootstrap over instances.



\section{Evaluation and Results}

\begin{table}[!htbp]
\centering
\caption{Evaluation results of baseline NL2SQL models on different datasets.}
\label{tab:evaluation-results}
\renewcommand{\arraystretch}{1.1}

% 缩放表格到页面宽度
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}l l c@{}}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{Accuracy (\%)} \\
\midrule
\textbf{Spider}  & \texttt{gaussalgo/T5-LM-Large-text2sql-spider} & 41.3 \\
\textbf{WikiSQL} & \texttt{mrm8488/t5-base-finetuned-wikiSQL}     & 56.0 \\
\bottomrule
\end{tabular}
}
\end{table}

To evaluate our NL2SQL system, we primarily use the \textbf{Spider} benchmark, as it exhibits the properties we aim to improve upon. As described in the data section, we preprocess the \textbf{Spider} dataset to generate the correct table schemas for our experiments. For evaluation, our approach directly compares the predicted SQL code with the gold (reference) SQL, while ignoring differences in spaces and minor variations such as the use of double quotes (``~'') versus single quotes (' '), since the functional equivalence of SQL queries is more important for practical use cases.

Due to the complex nature of the NL2SQL task, there are no simple random baselines. Therefore, for the \textbf{Spider} dataset, we selected \texttt{gaussalgo/T5-LM-Large-text2sql-spider}, a medium-sized model that is not state-of-the-art but achieves relatively strong performance. The \texttt{gaussalgo/T5-LM-Large-text2sql-spider} model is a fine-tuned version of Google’s \textbf{T5-Large} (Text-to-Text Transfer Transformer) language model, specifically adapted for the \textbf{Spider} text-to-SQL benchmark. Built on the encoder--decoder architecture of T5, it converts natural language questions into structured SQL queries by framing the task as a sequence-to-sequence generation problem. We evaluated the \texttt{gaussalgo/T5-LM-Large-text2sql-spider} model on the \textbf{Spider} dataset and achieved a score of 41.3\% accuracy.

Nevertheless, we also use the \textbf{WikiSQL} dataset to ensure that our model does not experience a performance drop on simpler NL2SQL tasks while focusing on multi-table generation. For evaluation, we adopt the same metrics as used for the \textbf{Spider} dataset, comparing the predicted SQL queries with the gold SQL and ignoring minor differences such as spacing or quotation marks. We evaluated the baseline model \texttt{mrm8488/t5-base-finetuned-wikiSQL} on this dataset and achieved an accuracy of 56\%. The \texttt{mrm8488/t5-base-finetuned-wikiSQL} model is a \textbf{T5-base} transformer fine-tuned on the \textbf{WikiSQL} benchmark, designed to translate natural language questions into SQL queries over single-table databases.



\section{Work Plan}
Our project spans a total of seven weeks, of which the first two have already been completed. In the initial phase, we conducted background research, finalized the methodology, selected datasets (Spider and BIRD), and evaluated several baseline NL2SQL models to establish reference performance. The remaining five weeks will focus on implementation, fine-tuning, and evaluation, following the plan below:

\begin{itemize}
  \item \textbf{Weeks 1 \& 2 (Completed):} 
  Conducted literature review and model planning. 
  
  Defined the overall methodology and model architecture, including graph-based schema design and reinforcement learning discussion. 
  
  Collected datasets (Spider and BIRD) and tested baseline models for initial performance comparison.

  \item \textbf{Week 3 (In-progress):} 
  Implement the schema-to-graph conversion module using the Hybrid Graph baseline. 
  
  Generate serialized graph representations for Spider databases and verify preprocessing correctness.

  \item \textbf{Week 4:} 
  Integrate graph-augmented inputs with Llama-8B.
  
  Begin LoRA fine-tuning on the Spider dataset, adjusting learning rate and rank parameters for stability.


  \item \textbf{Week 5:} 
  Extend experiments to include emantic Edge and Typed Graph variants.
  
  Compare their structural expressiveness and measure their influence on join reasoning accuracy.

  \item \textbf{Week 6:} 
  Try to implement and test Execution-Guided Decoding (EGD) for inference, if everything went smoothly.
  
  Evaluate models on Spider dev and BIRD subsets using metrics such as Exact Match (EM), Execution Accuracy (EX), and Join Accuracy (JAcc).

  \item \textbf{Week 7:} 
  Conduct full analysis and ablation studies across graph designs. 
  
  Integrating all conclusions into final report and presentation slides.
\end{itemize}

\noindent
We prioritizes completing the Hybrid Graph baseline and LoRA fine-tuning as the main deliverables. 

\usepackage{tcolorbox}
\tcbuselibrary{listings, skins, breakable}

\begin{tcolorbox}[
    title=Database Schema Overview,
    fonttitle=\bfseries,
    colback=blue!3,
    colframe=blue!60!black,
    colbacktitle=blue!15!white,
    enhanced,
    breakable,
    sharp corners,
]

\textbf{[DATABASE]} farm

\vspace{2mm}
\textbf{[TABLES]}
\begin{itemize}
    \item \texttt{city}: City\_ID, Official\_Name, Status, ...
    \item \texttt{farm}: Farm\_ID, Host\_city\_ID, Year, ...
    \item \texttt{farm\_competition}: Competition\_ID, Year, ...
\end{itemize}

\vspace{2mm}
\textbf{[FOREIGN KEYS]}
\begin{itemize}
    \item \texttt{farm.Host\_city\_ID -> city.City\_ID}
    \item \texttt{farm\_competition.Host\_city\_ID -> city.City\_ID}
\end{itemize}

\vspace{2mm}
\textbf{[SEMANTIC LINKS]}
\begin{itemize}
    \item \texttt{farm.Year ≈ farm\_competition.Year}
\end{itemize}

\end{tcolorbox}


% \bibliographystyle{plain}   
\bibliography{custom}

\newpage
\appendix
\section{Supplemental Material}
\subsection{Examples of Data}\label{app:exp}
\begin{lstlisting}[language=Python, caption={Example of Spider schema structure}, label={lst:spider_schema}]
{
    "db_id": "entrepreneur",
    "foreign_keys": [
        [2, 6]
    ],
    "primary_keys": [1, 6],
    "table_names": [
        "entrepreneur",
        "people"
    ],
    "table_names_original": [
        "entrepreneur",
        "people"
    ]
},
{
    "column_names": [
        [-1, "*"],
        [0, "conductor id"],
        [0, "name"],
        [0, "age"],
        [0, "nationality"],
        [0, "year of work"],
        [1, "orchestra id"],
        [1, "orchestra"],
        [1, "conductor id"],
        [1, "record company"],
        [1, "year of founded"],
        [1, "major record format"],
        [2, "performance id"],
        [2, "orchestra id"],
        [2, "type"],
        [2, "date"],
        [2, "official ratings (millions)"],
        [2, "weekly rank"],
        [2, "share"],
        [3, "show id"],
        [3, "performance id"],
        [3, "if first show"],
        [3, "result"],
        [3, "attendance"]
    ],
    "column_names_original": [
        [-1, "*"],
        [0, "Conductor_ID"],
        [0, "Name"],
        [0, "Age"],
        [0, "Nationality"],
        [0, "Year_of_Work"],
        [1, "Orchestra_ID"],
        [1, "Orchestra"],
        [1, "Conductor_ID"],
        [1, "Record_Company"],
        [1, "Year_of_Founded"],
        [1, "Major_Record_Format"],
        [2, "Performance_ID"],
        [2, "Orchestra_ID"],
        [2, "Type"],
        [2, "Date"],
        [2, "Official_ratings_(millions)"],
        [2, "Weekly_rank"],
        [2, "Share"],
        [3, "Show_ID"],
        [3, "Performance_ID"],
        [3, "If_first_show"],
        [3, "Result"],
        [3, "Attendance"]
    ]
}
\end{lstlisting}
\subsection{Code for baseline}
\begin{lstlisting}[language=Python, caption={Code for baseline model evaluation on Spider}, label={lst:baseline_spider}]
!rm -rf spider
!git clone https://github.com/taoyds/spider.git

import json

# Input: Spider-style schema list
IN_PATH = "/content/spider/evaluation_examples/examples/tables.json"
OUT_PATH = "tables_by_db.json"

def main():
    # Load the list of database schema dictionaries
    with open(IN_PATH, "r") as f:
        schema_list = json.load(f)

    # Convert list → dict keyed by db_id
    db_dict = {entry["db_id"]: entry for entry in schema_list if "db_id" in entry}

    # Save the dict as JSON
    with open(OUT_PATH, "w") as f:
        json.dump(db_dict, f, indent=2)

    print(f"Converted {len(db_dict)} databases into {OUT_PATH}")

if __name__ == "__main__":
    main()

# --- installs (for Colab / first time) ---
# !pip install -q transformers datasets tqdm

import re
import json
from typing import Optional, List

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

# =========================
# 1. Config & device
# =========================

MODEL_NAME = "gaussalgo/T5-LM-Large-text2sql-spider"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

TABLES_JSON_PATH = "tables_by_db.json"
MAX_EXAMPLES: Optional[int] = 1034
print("Using device:", DEVICE)

# =========================
# 2. Load Spider dataset
# =========================

ds = load_dataset("xlangai/spider")
train = ds["train"]
dev = ds["validation"]

if MAX_EXAMPLES is not None:
    dev = dev.select(range(MAX_EXAMPLES))

print(f"Loaded {len(dev)} dev examples")

# =========================
# 3. Load schema dict
# =========================

with open(TABLES_JSON_PATH, "r") as f:
    db_schemas = json.load(f)
print(f"Loaded schema dict for {len(db_schemas)} databases")

def build_schema_string(db_id: str) -> str:
    """Convert a single db_id schema dict into a readable schema string."""
    db = db_schemas.get(db_id)
    if db is None:
        return ""
    table_names = db.get("table_names_original", [])
    col_names = db.get("column_names_original", [])
    col_types = db.get("column_types", [])
    primary_keys = set(db.get("primary_keys", []))

    from collections import defaultdict
    cols_by_table = defaultdict(list)
    for col_idx, (t_idx, col_name) in enumerate(col_names):
        if t_idx == -1:
            continue
        dtype = col_types[col_idx] if col_idx < len(col_types) else "text"
        cols_by_table[t_idx].append((col_idx, col_name, dtype))

    table_strings = []
    for t_idx, t_name in enumerate(table_names):
        parts = [f"\"{t_name}\""]
        for col_idx, col_name, dtype in cols_by_table[t_idx]:
            parts.append(f" \"{col_name}\" {dtype} ,")
        pk_names = [
            col_names[pk_idx][1]
            for pk_idx in primary_keys
            if col_names[pk_idx][0] == t_idx
        ]
        if pk_names:
            parts.append("primary key: " + ", ".join(f"\"{n}\"" for n in pk_names))
        table_strings.append(" ".join(parts))
    return " [SEP] ".join(table_strings)

# =========================
# 4. Load model & tokenizer
# =========================

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
model.to(DEVICE)
model.eval()

@torch.no_grad()
def generate_sql(question: str, db_id: str, max_new_tokens: int = 128) -> str:
    """Generate SQL given a question and db_id, using its schema."""
    schema_str = build_schema_string(db_id)
    if schema_str:
        input_text = f"Question: {question} Schema: {schema_str}"
    else:
        input_text = f"Question: {question} Database: {db_id}"
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
    ).to(DEVICE)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        num_beams=4,
        early_stopping=True,
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

# =========================
# 5. Normalize & evaluate
# =========================

def normalize_sql(sql: str) -> str:
    sql = sql.strip().rstrip(";")
    sql = re.sub(r"\s+", " ", sql)
    sql = sql.lower()
    sql = sql.replace('"', "'")
    sql = sql.replace(" ", "")
    return sql

lf_correct = 0
n = 0
inspect: List[dict] = []

for ex in tqdm(dev):
    question = ex["question"]
    db_id = ex["db_id"]
    gold_sql = ex["query"]

    pred_sql = generate_sql(question, db_id)
    lf_ok = normalize_sql(pred_sql) == normalize_sql(gold_sql)

    if lf_ok:
        lf_correct += 1
    if len(inspect) < 10:
        inspect.append({
            "question": question,
            "db_id": db_id,
            "gold_sql": gold_sql,
            "pred_sql": pred_sql,
            "lf_ok": lf_ok,
        })
    n += 1

acc = lf_correct / n if n else 0
print(f"\nEvaluated on {n} dev examples")
print(f"Logical-form accuracy: {acc:.4f}")

print("\n=== Sample predictions ===")
for i, item in enumerate(inspect, 1):
    print(f"\nExample {i}")
    print("DB:", item["db_id"])
    print("Question:", item["question"])
    print("GOLD SQL:", item["gold_sql"])
    print("PRED SQL:", item["pred_sql"])
    print("LF match:", item["lf_ok"])
\end{lstlisting}








% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.


\end{document}
