\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{Multi-table NL2SQL}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Zhanhao Liu \\
  University of Michigan \\
  Ann Arbor, MI \\
  \texttt{zhanhaol@umich.edu} \\\And
  Qiulin Fan \\
  University of Michigan \\
  Ann Arbor, MI \\
  \texttt{
rynnefan@umich.edu} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\section{Project Goals}
The goal of this project is to enhance NL2SQL models’ ability to generate accurate multi-table SQL queries by incorporating graph-based schema representations and reinforcement learning. Traditionally, relational databases are accessed through manually written SQL queries, which require considerable technical expertise. With the emergence of Large Language Models (LLMs), NL2SQL has become increasingly feasible and effective, enabling the automatic translation of natural language into SQL queries.

However, current NL2SQL systems often struggle with multi-table queries, where understanding relationships between tables and correctly forming join conditions are especially challenging. Executing join operations across multiple tables can lead to Cartesian space complexity, making it critical to minimize both the search space and the computational cost of query generation. By leveraging graph structures to explicitly model schema connections and reinforcement learning to reward correct query construction while reducing time and space complexity, this project aims to improve both the accuracy and efficiency of multi-table query generation.

This problem matters because natural language interfaces to databases make data more accessible to non-technical users. If multi-table NL2SQL systems become more reliable, data analysts, researchers, and business professionals could query complex databases using plain language without needing SQL expertise. It would affect both academic contexts and industry

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{image/bird.png}
    \caption{An example text-to-SQL task from the BIRD dataset. The query involves multiple tables, each with columns containing realistic values (not fully shown here).}
    \label{fig:bird}
\end{figure*}
\section{NLP Task Definition}

The specific NLP task this project will address is multi-table Natural Language to SQL (NL2SQL) generation. The task is to take a natural language query as input and automatically produce a syntactically correct and semantically accurate SQL query that retrieves the correct result from a relational database containing multiple interconnected tables.
At inference time :\\
Inputs:

1.A natural language query provided by the user

2.A relational database schema, which may contain multiple tables with foreign key relationships

Note: A foreign key is a column (or set of columns) in one table that refers to the primary key of another table.

Outputs:

A structured SQL query that can be executed on the target database to return the intended result.\\
\textbf{Scope Clarification (What the project is not)}:

This project focuses specifically on improving the performance of SQL generation in multi-table scenarios. It will not address single-table NL2SQL, as this area is comparatively well studied and current models already achieve strong performance. In addition, the system will not attempt to handle database administration tasks, such as maintaining, securing, or optimizing database system. Finally, the project is not intended to solve open-domain question answering; instead, it will focus on natural language queries that are specific, concrete, and directly mappable to relational database operations.\\
\textbf{Current Idea}:

Our current approach begins with generating a graph representation of the database. This graph is provided to the model alongside the database schema and the natural language (NL) query, both during training and inference.

To construct the complete graph, we first construct a graph for each table in the database individually, and then connect these graphs by adding edges between tables. The connections will be established either through explicit foreign key relationships provided in the schema or through semantic understanding of column names (e.g., “Birthday” in one table likely corresponds to “DOB” in another). The graphs for individual tables will remain relatively sparse. To illustrate this approach, we will provide an example of our intended graph structure.
\begin{center}
    \includegraphics[width=0.5\textwidth]{image/graph.png}
\end{center}

Now the graph captures richer representations of the relationships between tables.  We will incorporate this graph representation alongside the original inputs (schema and query) to further train the NL2SQL model using reinforcement learning.
The training process will leverage three types of feedback: (1) compilability, measuring whether the generated SQL can be executed successfully on the database, (2) accuracy, assessing whether the query retrieves the correct results, and (3) optimization with respect to time and space complexity. 

Our proposed idea is based on the assumption that incorporating graph representations during training will help the model better capture relationships between tables, thereby enabling it to generate more accurate and reasonable SQL queries that perform "join" between tables.

\section{Data}
We wll be using the Spider and BIRD-SQL datasests to train and evaluate our model.\\\\
\textbf{1. Spider dataset}:
Spider is a large-scale, cross-domain Text-to-SQL dataset designed to evaluate models' ability to generalize to unseen databases. It contains over 10,000 natural language questions and corresponding SQL queries across 200 databases in 138 domains. Unlike simpler datasets such as WikiSQL, Spider emphasizes compositional generalization: the training and test splits do not share databases, requiring models to handle new schemas at test time. The queries span a wide range of SQL components, including joins, nested queries, group by, and order by. We will use Spider 1.0 and 2.0, to compare our model's compositional generalization with the model before our fine tuning, and the state-of-the-art models in NL2SQL.\\\\
\textbf{2. BIRD-SQL dataset}:
BIRD-SQL is designed to benchmark systems that map natural language questions into SQL queries, with a strong emphasis on realistic, large-scale relational databases. The dataset contains over 12,751 unique question-SQL pairs across 95 large databases totaling 33.4 GB of data. It spans more than 37 professional domains, including blockchain, hockey, healthcare, education, and more.
Unlike earlier benchmarks, which typically provide only database schemas with a few rows of clean, well-curated column names, BIRD emphasizes reasoning over the actual contents of tables. This means that generating the correct SQL query often requires understanding and leveraging the values stored in the database, not just the schema. To better reflect real-world conditions, BIRD incorporates large and noisy values that mimic practical challenges in database usage. An example of Bird data is shown in Figure 1.
\subsection{data set usage}
Taking Spider dataset as an example:\\
Each training instance consists of a natural language question paired with its database schema and the corresponding SQL query. The original Spider dataset provides \texttt{train.json}/\texttt{dev.json} for question–SQL pairs and \texttt{tables.json} for schema information. In preprocessing, we join them by database ID: for each question, we retrieve its database schema (tables, columns, and foreign keys) and serialize it into a structured text format. We then concatenate the natural language question with the schema description to form the model input, and the SQL query serves as the target output. This yields a training dataset in the form of \texttt{(NL + schema) → SQL}, which can be directly used for fine-tuning large language models.

% Compare our performance with 7B baseline

\section{Related Work}
The three papers included here are closely related to our project. One gives a gateway to existing methods in the field of NL2SQL, one comprehensively describes the performance of existing models, and the other is an important example of an existing pipeline that has reached great success in this area (by lifting the performance of a 7B-parameter open source model to has similar performance on NL2SQL as standard GPT-4).
\begin{itemize}
    \item A comprehensive overview of recent progress in Text-to-SQL has been provided by Liu et al. in their survey \cite{liu2025surveytexttosqlerallms}. This work reviews the evolution of Text-to-SQL systems in the era of large language models (LLMs), organizing the field along the entire lifecycle of the task: model design, data construction, evaluation, and error analysis. It emphasizes the importance of schema linking, database content retrieval, and post-processing modules such as execution-guided correction. The survey provides a foundation for understanding existing approaches and motivates the need for novel methods that combine structured schema representations with LLM capabilities. 
It greatly inspires our project, as it categorizes encoding strategies (e.g., sequential, graph-based, and separate encoding) and decoding strategies (e.g., greedy search, beam search, constraint-aware decoding), highlighting how these choices affect robustness and accuracy \cite{liu2025surveytexttosqlerallms}. We gain inspirations from these methods to develop our own.\\


\item Spider 2.0 was recently introduced by Lei et al. \cite{lei2025spider20evaluatinglanguage} as a new benchmark for evaluating language models on real-world enterprise text-to-SQL workflows. Unlike earlier datasets such as Spider 1.0 and BIRD, which primarily involve relatively small schemas and simplified queries, Spider 2.0 incorporates enterprise-grade databases containing thousands of columns, multiple schemas, and terabyte-scale data. The benchmark covers 632 workflow problems derived from industrial use cases, requiring models not only to generate SQL but also to handle dialect-specific functions, integrate project-level codebases, and dynamically interact with database environments. Experimental results demonstrate a substantial performance gap: while GPT-based models achieve over 90\% execution accuracy on Spider 1.0, success rates drop to around 21\% on Spider 2.0, underscoring the limitations of current LLMs in realistic settings. This work highlights critical challenges for text-to-SQL research, including schema linking in extremely large databases, managing SQL dialect variations, and reasoning across multi-step analytical tasks, thus motivating new approaches that move beyond static parsing into agentic, workflow-aware modeling.\\

\item LearNAT was proposed by Liao et al. \cite{liao2025learnatlearningnl2sqlastguided} as a framework to improve NL2SQL performance of open-source LLMs through task decomposition and reinforcement learning. Unlike prior methods that rely heavily on closed-source models with prompt engineering, LearNAT leverages Abstract Syntax Trees (ASTs) to guide query decomposition into subtasks, enabling more interpretable reasoning over complex SQL structures. The framework introduces three key components: (1) a Decomposition Synthesis Procedure that uses AST-guided search and pruning to generate valid subtasks, (2) Margin-Aware Reinforcement Learning with AST-based preference signals to optimize multi-step reasoning, and (3) Adaptive Demonstration Reasoning to dynamically retrieve relevant examples during inference. 
Experiments on Spider and BIRD show that LearNAT enables a 7B-parameter open-source model to approach GPT-4 performance, narrowing the performance gap between open- and closed-source systems. This work demonstrates the effectiveness of combining AST-guided decomposition with reinforcement learning, and provides inspiration for our project design. Current draft of our plan uses a graph-based decomposition representation of the input schema.


\end{itemize}

\section{Evaluation}
For evaluation, we will consider several complementary metrics. First, we note that Exact Match on SQL generation is too restrictive, since there are often multiple equivalent ways to write SQL queries that produce the same correct result, so we will not rely on this metric. Instead, we will use Execution Accuracy, which runs both the predicted and gold queries on the same database instance and compares the outputs, focusing on functional correctness rather than syntax. We will also measure the Executable Rate, for example, whether the generated SQL can be successfully parsed and executed without error, as well as the runtime performance of each query to capture efficiency in terms of time cost. However, in multi-table NL2SQL, these measures alone are not sufficient, as a query may return the right answer by accident while using incorrect join paths or including unnecessary tables. To address this, we propose an additional metric, Structure/Join Accuracy, which evaluates whether the predicted SQL selects the correct set of tables and applies the correct join conditions to match the gold query. Together, these metrics provide a comprehensive evaluation of correctness, robustness, and efficiency. To perform this evaluation, our naive approach is to parse the join operations from both the predicted and gold SQL queries, then compare the pairs of tables that are joined together, as well as the total number of joins used.\\\\
\textbf{\Large Baseline}
For our baseline, we will start from existing state-of-the-art NL2SQL models that have been widely evaluated on datasets such as Spider and BIRD-SQL. In particular, we consider two categories:

\begin{itemize}
    \item \textbf{Grammar-based/Encoder–Decoder models}: Approaches such as Seq2SQL and IRNet, which map natural language to SQL using encoder–decoder architectures and schema linking mechanisms. These methods provide strong supervised baselines and are well-studied in the literature.
    \item \textbf{Pre-trained language model based approaches}: More recent models (e.g., PICARD, RAT-SQL + BERT, or LLaMA-based text-to-SQL finetunes) that leverage large pre-trained transformers with schema serialization. These models achieve high accuracy on Spider and represent the current benchmark for NL2SQL.
\end{itemize}

We will deploy representative implementations from these categories and evaluate their performance on our chosen datasets. This evaluation establishes a reference point against which we can compare our proposed method. The most suitable baseline model, in terms of both performance and implementation feasibility, will be selected as the foundation for our fine-tuning experiments.


\section{Plan}
Our workflow should be aligned with the following steps:\\
1. Deploy existing NL2SQL models identified in related work.\\
2. Evaluate baseline performance: Run these models on our evaluation dataset to obtain performance metrics and determine the most suitable baseline model for finetuning.\\
3. Design graph representations: Research and develop an algorithm to extract graph-based representations of the dataset.\\
4. Integrate graph inputs: Modify the selected baseline model to accept the graph representation as an additional input.\\
5. Fine-tune the model: Apply techniques such as LoRA (Low-Rank Adaptation) to efficiently fine-tune the model.\\
6. Evaluate and compare: Assess the performance of the fine-tuned model and compare it against existing baselines.\\


\section{Multi-person Team Justification}
We have two people in our team, and we think the work we proposed requires two peroson to work together. Overall work includes: (1) Dataset processing (from Spider and BIRD-SQL data to our input representation); (2) Schema integration (implementing data augmentation strategies, NL-schema connection strategies); (3) Model fine tuning implementation (through LoRA) and training; (4) Evaluation through our baseline above. These tasks require substantial NLP work in adapting LLMs to text-to-SQL tasks and analyzing their natural language reasoning errors.\\\\
Summarize above, there are two main components of this project: database to graph conversion and finetuning. In the research phase, Zhanhao Liu will focus on developing techniques for converting databases into graphs, while Qiulin Fan will focus on implementing model fine-tuning strategies. And we will evaluate our fine-tuned model and improve it through comparisons, together. Although our primary responsibilities are divided, we will both explore literature in both areas and complement each other’s work.




\bibliographystyle{plain}   
\bibliography{custom}


% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.


\end{document}
